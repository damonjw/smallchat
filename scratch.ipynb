{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "42230347",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "import dotenv\n",
    "import litellm\n",
    "from pprint import pprint\n",
    "dotenv.load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0e414d79",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['ANTHROPIC_API_KEY']"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "keys = ['OPENAI_API_KEY', 'GEMINI_API_KEY', 'XAI_API_KEY', 'ANTHROPIC_API_KEY']\n",
    "[k for k in keys if k in os.environ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1686a943",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'keys_in_environment': True, 'missing_keys': []}"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "litellm.validate_environment(\"anthropic/sonnet-4-5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e17412d3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['anthropic/claude-opus-4-5-20251101',\n",
       " 'anthropic/claude-haiku-4-5-20251001',\n",
       " 'anthropic/claude-sonnet-4-5-20250929',\n",
       " 'anthropic/claude-opus-4-1-20250805',\n",
       " 'anthropic/claude-opus-4-20250514',\n",
       " 'anthropic/claude-sonnet-4-20250514',\n",
       " 'anthropic/claude-3-7-sonnet-20250219',\n",
       " 'anthropic/claude-3-5-haiku-20241022',\n",
       " 'anthropic/claude-3-haiku-20240307',\n",
       " 'anthropic/claude-3-opus-20240229']"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "litellm.get_valid_models(check_provider_endpoint=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "e97a526d",
   "metadata": {},
   "outputs": [],
   "source": [
    "response = litellm.completion(\n",
    "  model= \"anthropic/claude-sonnet-4-5\",\n",
    "  tools = [],\n",
    "  messages= [{'role':'user', \"content\": \"What do I get if I mogrify a peanut?\"}, \n",
    "             {'role':'assistant', 'content': \"I need more context about what mogrify might mean in this setting\"},\n",
    "             {'role':'user', \"content\": \"Interpret it using the Schartz-Metterklume method\"},\n",
    "             {'role':'assistant', 'content': \"Is this a word game?\"},\n",
    "             {'role':'assistant', 'content': \"I'll assume it is!\"},\n",
    "             {'role':'system', 'content': \"Be proactive and strident\"}\n",
    "             ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "a1dc174e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
      "LiteLLM.Info: If you need to debug this error, use `litellm._turn_on_debug()'.\n",
      "\n"
     ]
    },
    {
     "ename": "BadRequestError",
     "evalue": "litellm.BadRequestError: AnthropicException - litellm.BadRequestError: Anthropic requires at least one non-system message. Either provide one, or set `litellm.modify_params = True` // `litellm_settings::modify_params: True` to add the dummy user message - {'role': 'user', 'content': 'Please continue.'}.\nReceived Messages=[]",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mBadRequestError\u001b[0m                           Traceback (most recent call last)",
      "File \u001b[0;32m/usr/local/lib/python3.12/dist-packages/litellm/llms/anthropic/chat/transformation.py:778\u001b[0m, in \u001b[0;36mAnthropicConfig.transform_request\u001b[0;34m(self, model, messages, optional_params, litellm_params, headers)\u001b[0m\n\u001b[1;32m    777\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 778\u001b[0m     anthropic_messages \u001b[38;5;241m=\u001b[39m \u001b[43manthropic_messages_pt\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    779\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    780\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmessages\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmessages\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    781\u001b[0m \u001b[43m        \u001b[49m\u001b[43mllm_provider\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43manthropic\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    782\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    783\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "File \u001b[0;32m/usr/local/lib/python3.12/dist-packages/litellm/litellm_core_utils/prompt_templates/factory.py:1877\u001b[0m, in \u001b[0;36manthropic_messages_pt\u001b[0;34m(messages, model, llm_provider)\u001b[0m\n\u001b[1;32m   1876\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m litellm\u001b[38;5;241m.\u001b[39mmodify_params:\n\u001b[0;32m-> 1877\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m litellm\u001b[38;5;241m.\u001b[39mBadRequestError(\n\u001b[1;32m   1878\u001b[0m         message\u001b[38;5;241m=\u001b[39m\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAnthropic requires at least one non-system message. Either provide one, or set `litellm.modify_params = True` // `litellm_settings::modify_params: True` to add the dummy user message - \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mDEFAULT_USER_CONTINUE_MESSAGE_TYPED\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m   1879\u001b[0m         model\u001b[38;5;241m=\u001b[39mmodel,\n\u001b[1;32m   1880\u001b[0m         llm_provider\u001b[38;5;241m=\u001b[39mllm_provider,\n\u001b[1;32m   1881\u001b[0m     )\n\u001b[1;32m   1882\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "\u001b[0;31mBadRequestError\u001b[0m: litellm.BadRequestError: Anthropic requires at least one non-system message. Either provide one, or set `litellm.modify_params = True` // `litellm_settings::modify_params: True` to add the dummy user message - {'role': 'user', 'content': 'Please continue.'}.",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mAnthropicError\u001b[0m                            Traceback (most recent call last)",
      "File \u001b[0;32m/usr/local/lib/python3.12/dist-packages/litellm/main.py:2331\u001b[0m, in \u001b[0;36mcompletion\u001b[0;34m(model, messages, timeout, temperature, top_p, n, stream, stream_options, stop, max_completion_tokens, max_tokens, modalities, prediction, audio, presence_penalty, frequency_penalty, logit_bias, user, reasoning_effort, verbosity, response_format, seed, tools, tool_choice, logprobs, top_logprobs, parallel_tool_calls, web_search_options, deployment_id, extra_headers, safety_identifier, service_tier, functions, function_call, base_url, api_version, api_key, model_list, thinking, shared_session, **kwargs)\u001b[0m\n\u001b[1;32m   2327\u001b[0m     verbose_logger\u001b[38;5;241m.\u001b[39mdebug(\n\u001b[1;32m   2328\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mLITELLM_ANTHROPIC_DISABLE_URL_SUFFIX is set, skipping /v1/messages suffix\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   2329\u001b[0m     )\n\u001b[0;32m-> 2331\u001b[0m response \u001b[38;5;241m=\u001b[39m \u001b[43manthropic_chat_completions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcompletion\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   2332\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2333\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmessages\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmessages\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2334\u001b[0m \u001b[43m    \u001b[49m\u001b[43mapi_base\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mapi_base\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2335\u001b[0m \u001b[43m    \u001b[49m\u001b[43macompletion\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43macompletion\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2336\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcustom_prompt_dict\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlitellm\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcustom_prompt_dict\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2337\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmodel_response\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmodel_response\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2338\u001b[0m \u001b[43m    \u001b[49m\u001b[43mprint_verbose\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mprint_verbose\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2339\u001b[0m \u001b[43m    \u001b[49m\u001b[43moptional_params\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moptional_params\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2340\u001b[0m \u001b[43m    \u001b[49m\u001b[43mlitellm_params\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlitellm_params\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2341\u001b[0m \u001b[43m    \u001b[49m\u001b[43mlogger_fn\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlogger_fn\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2342\u001b[0m \u001b[43m    \u001b[49m\u001b[43mencoding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mencoding\u001b[49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# for calculating input/output tokens\u001b[39;49;00m\n\u001b[1;32m   2343\u001b[0m \u001b[43m    \u001b[49m\u001b[43mapi_key\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mapi_key\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2344\u001b[0m \u001b[43m    \u001b[49m\u001b[43mlogging_obj\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlogging\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2345\u001b[0m \u001b[43m    \u001b[49m\u001b[43mheaders\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mheaders\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2346\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtimeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2347\u001b[0m \u001b[43m    \u001b[49m\u001b[43mclient\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mclient\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2348\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcustom_llm_provider\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcustom_llm_provider\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2349\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2350\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m optional_params\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstream\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mFalse\u001b[39;00m) \u001b[38;5;129;01mor\u001b[39;00m acompletion \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[1;32m   2351\u001b[0m     \u001b[38;5;66;03m## LOGGING\u001b[39;00m\n",
      "File \u001b[0;32m/usr/local/lib/python3.12/dist-packages/litellm/llms/anthropic/chat/handler.py:338\u001b[0m, in \u001b[0;36mAnthropicChatCompletion.completion\u001b[0;34m(self, model, messages, api_base, custom_llm_provider, custom_prompt_dict, model_response, print_verbose, encoding, api_key, logging_obj, optional_params, timeout, litellm_params, acompletion, logger_fn, headers, client)\u001b[0m\n\u001b[1;32m    334\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    335\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mProvider config not found for model: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmodel\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m and provider: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mcustom_llm_provider\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    336\u001b[0m     )\n\u001b[0;32m--> 338\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[43mconfig\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtransform_request\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    339\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    340\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmessages\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmessages\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    341\u001b[0m \u001b[43m    \u001b[49m\u001b[43moptional_params\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moptional_params\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    342\u001b[0m \u001b[43m    \u001b[49m\u001b[43mlitellm_params\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlitellm_params\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    343\u001b[0m \u001b[43m    \u001b[49m\u001b[43mheaders\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mheaders\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    344\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    346\u001b[0m \u001b[38;5;66;03m## LOGGING\u001b[39;00m\n",
      "File \u001b[0;32m/usr/local/lib/python3.12/dist-packages/litellm/llms/anthropic/chat/transformation.py:784\u001b[0m, in \u001b[0;36mAnthropicConfig.transform_request\u001b[0;34m(self, model, messages, optional_params, litellm_params, headers)\u001b[0m\n\u001b[1;32m    783\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m--> 784\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m AnthropicError(\n\u001b[1;32m    785\u001b[0m         status_code\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m400\u001b[39m,\n\u001b[1;32m    786\u001b[0m         message\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mReceived Messages=\u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(\u001b[38;5;28mstr\u001b[39m(e), messages),\n\u001b[1;32m    787\u001b[0m     )  \u001b[38;5;66;03m# don't use verbose_logger.exception, if exception is raised\u001b[39;00m\n\u001b[1;32m    789\u001b[0m \u001b[38;5;66;03m## Add code_execution tool if container_upload is in messages\u001b[39;00m\n",
      "\u001b[0;31mAnthropicError\u001b[0m: litellm.BadRequestError: Anthropic requires at least one non-system message. Either provide one, or set `litellm.modify_params = True` // `litellm_settings::modify_params: True` to add the dummy user message - {'role': 'user', 'content': 'Please continue.'}.\nReceived Messages=[]",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mBadRequestError\u001b[0m                           Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[19], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m response \u001b[38;5;241m=\u001b[39m \u001b[43mlitellm\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcompletion\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m      2\u001b[0m \u001b[43m  \u001b[49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43manthropic/claude-sonnet-4-5\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m      3\u001b[0m \u001b[43m  \u001b[49m\u001b[43mtools\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43m[\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      4\u001b[0m \u001b[43m  \u001b[49m\u001b[43mmessages\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43m[\u001b[49m\u001b[43m]\u001b[49m\n\u001b[1;32m      5\u001b[0m \u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/lib/python3.12/dist-packages/litellm/utils.py:1381\u001b[0m, in \u001b[0;36mclient.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m   1377\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m logging_obj:\n\u001b[1;32m   1378\u001b[0m     logging_obj\u001b[38;5;241m.\u001b[39mfailure_handler(\n\u001b[1;32m   1379\u001b[0m         e, traceback_exception, start_time, end_time\n\u001b[1;32m   1380\u001b[0m     )  \u001b[38;5;66;03m# DO NOT MAKE THREADED - router retry fallback relies on this!\u001b[39;00m\n\u001b[0;32m-> 1381\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m e\n",
      "File \u001b[0;32m/usr/local/lib/python3.12/dist-packages/litellm/utils.py:1250\u001b[0m, in \u001b[0;36mclient.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m   1248\u001b[0m         print_verbose(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mError while checking max token limit: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mstr\u001b[39m(e)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m   1249\u001b[0m \u001b[38;5;66;03m# MODEL CALL\u001b[39;00m\n\u001b[0;32m-> 1250\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[43moriginal_function\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1251\u001b[0m end_time \u001b[38;5;241m=\u001b[39m datetime\u001b[38;5;241m.\u001b[39mdatetime\u001b[38;5;241m.\u001b[39mnow()\n\u001b[1;32m   1252\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m _is_streaming_request(\n\u001b[1;32m   1253\u001b[0m     kwargs\u001b[38;5;241m=\u001b[39mkwargs,\n\u001b[1;32m   1254\u001b[0m     call_type\u001b[38;5;241m=\u001b[39mcall_type,\n\u001b[1;32m   1255\u001b[0m ):\n",
      "File \u001b[0;32m/usr/local/lib/python3.12/dist-packages/litellm/main.py:3774\u001b[0m, in \u001b[0;36mcompletion\u001b[0;34m(model, messages, timeout, temperature, top_p, n, stream, stream_options, stop, max_completion_tokens, max_tokens, modalities, prediction, audio, presence_penalty, frequency_penalty, logit_bias, user, reasoning_effort, verbosity, response_format, seed, tools, tool_choice, logprobs, top_logprobs, parallel_tool_calls, web_search_options, deployment_id, extra_headers, safety_identifier, service_tier, functions, function_call, base_url, api_version, api_key, model_list, thinking, shared_session, **kwargs)\u001b[0m\n\u001b[1;32m   3771\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m response\n\u001b[1;32m   3772\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m   3773\u001b[0m     \u001b[38;5;66;03m## Map to OpenAI Exception\u001b[39;00m\n\u001b[0;32m-> 3774\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[43mexception_type\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   3775\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3776\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcustom_llm_provider\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcustom_llm_provider\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3777\u001b[0m \u001b[43m        \u001b[49m\u001b[43moriginal_exception\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43me\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3778\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcompletion_kwargs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3779\u001b[0m \u001b[43m        \u001b[49m\u001b[43mextra_kwargs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3780\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/lib/python3.12/dist-packages/litellm/litellm_core_utils/exception_mapping_utils.py:2328\u001b[0m, in \u001b[0;36mexception_type\u001b[0;34m(model, original_exception, custom_llm_provider, completion_kwargs, extra_kwargs)\u001b[0m\n\u001b[1;32m   2326\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m exception_mapping_worked:\n\u001b[1;32m   2327\u001b[0m     \u001b[38;5;28msetattr\u001b[39m(e, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlitellm_response_headers\u001b[39m\u001b[38;5;124m\"\u001b[39m, litellm_response_headers)\n\u001b[0;32m-> 2328\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\n\u001b[1;32m   2329\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   2330\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m error_type \u001b[38;5;129;01min\u001b[39;00m litellm\u001b[38;5;241m.\u001b[39mLITELLM_EXCEPTION_TYPES:\n",
      "File \u001b[0;32m/usr/local/lib/python3.12/dist-packages/litellm/litellm_core_utils/exception_mapping_utils.py:638\u001b[0m, in \u001b[0;36mexception_type\u001b[0;34m(model, original_exception, custom_llm_provider, completion_kwargs, extra_kwargs)\u001b[0m\n\u001b[1;32m    633\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m (\n\u001b[1;32m    634\u001b[0m     original_exception\u001b[38;5;241m.\u001b[39mstatus_code \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m400\u001b[39m\n\u001b[1;32m    635\u001b[0m     \u001b[38;5;129;01mor\u001b[39;00m original_exception\u001b[38;5;241m.\u001b[39mstatus_code \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m413\u001b[39m\n\u001b[1;32m    636\u001b[0m ):\n\u001b[1;32m    637\u001b[0m     exception_mapping_worked \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[0;32m--> 638\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m BadRequestError(\n\u001b[1;32m    639\u001b[0m         message\u001b[38;5;241m=\u001b[39m\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAnthropicException - \u001b[39m\u001b[38;5;132;01m{\u001b[39;00merror_str\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m    640\u001b[0m         model\u001b[38;5;241m=\u001b[39mmodel,\n\u001b[1;32m    641\u001b[0m         llm_provider\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124manthropic\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m    642\u001b[0m     )\n\u001b[1;32m    643\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m original_exception\u001b[38;5;241m.\u001b[39mstatus_code \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m404\u001b[39m:\n\u001b[1;32m    644\u001b[0m     exception_mapping_worked \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "\u001b[0;31mBadRequestError\u001b[0m: litellm.BadRequestError: AnthropicException - litellm.BadRequestError: Anthropic requires at least one non-system message. Either provide one, or set `litellm.modify_params = True` // `litellm_settings::modify_params: True` to add the dummy user message - {'role': 'user', 'content': 'Please continue.'}.\nReceived Messages=[]"
     ]
    }
   ],
   "source": [
    "response = litellm.completion(\n",
    "  model= \"anthropic/claude-sonnet-4-5\",\n",
    "  tools = [],\n",
    "  messages= []\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "ce564620",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "If I apply the \"Schartz-Metterklume method\" (which sounds delightfully absurd and possibly references Saki's short story \"The Schartz-Metterklume Method\"), I might interpret \"mogrify\" as a whimsical transformation.\n",
      "\n",
      "So mogrifying a peanut could yield:\n",
      "\n",
      "- **A pea-nut** → Split it into a pea and a nut (literal deconstruction)\n",
      "- **A peanut** → **A teenup** (anagram)\n",
      "- **Peanut butter** (the natural evolution/transformation)\n",
      "- **An elephant** (reverse the old riddle: \"What's large and grey and comes in peanuts?\")\n",
      "- **George Washington Carver's notebook** (transforming the object into its historical significance)\n",
      "\n",
      "What method of mogrification were you actually thinking of? I'm genuinely curious about the rules here!\n"
     ]
    }
   ],
   "source": [
    "print(response.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "652362c7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe1740f3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "b12ffff0",
   "metadata": {},
   "outputs": [],
   "source": [
    "tool1 = {\n",
    "        \"type\": \"function\",\n",
    "        \"function\": {\n",
    "            \"name\": 'mogrify_string',\n",
    "            \"description\": 'Mogrifies a string, according to the specified format',\n",
    "            \"parameters\": {\n",
    "                'type': 'object',\n",
    "                'properties': {\n",
    "                    's': {'description': \"String to be mogrified\", 'type': 'string'},\n",
    "                    'fmt': {'description': \"Format to mogrify with\", 'type': 'string'}\n",
    "                }\n",
    "            },\n",
    "            'required': ['s','fmt']\n",
    "        }\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "61349c33",
   "metadata": {},
   "outputs": [],
   "source": [
    "response = litellm.completion(\n",
    "  model= \"anthropic/claude-sonnet-4-5\",\n",
    "  tools = [tool1],\n",
    "  messages= [{'role':'user', \"content\": \"What do I get if I mogrify the string `hello` according to format `foo3`?\"}]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "83522ab7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'content': None,\n",
      " 'function_call': None,\n",
      " 'provider_specific_fields': {'citations': None, 'thinking_blocks': None},\n",
      " 'role': 'assistant',\n",
      " 'tool_calls': [{'function': {'arguments': '{\"s\": \"hello\", \"fmt\": \"foo3\"}',\n",
      "                              'name': 'mogrify_string'},\n",
      "                 'id': 'toolu_018Y8HNyDxC1gGZNjdxNze6d',\n",
      "                 'index': 0,\n",
      "                 'type': 'function'}]}\n"
     ]
    }
   ],
   "source": [
    "pprint(response.choices[0].message.model_dump())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "4ef6933b",
   "metadata": {},
   "outputs": [],
   "source": [
    "response0 = litellm.completion(\n",
    "  model= \"anthropic/claude-sonnet-4-5\",\n",
    "  tools = [tool1],\n",
    "  messages= [{'role':'user', \"content\": \"What shall I do now?\"}]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "fee4e634",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'content': \"I don't have enough context to give you specific advice about \"\n",
      "            'what you should do now. That depends entirely on your situation, '\n",
      "            \"goals, and what you're trying to accomplish!\\n\"\n",
      "            '\\n'\n",
      "            'However, I can help you if you:\\n'\n",
      "            \"- Tell me more about what you're working on or what decision \"\n",
      "            \"you're facing\\n\"\n",
      "            '- Need help with string formatting/manipulation using the '\n",
      "            'mogrify_string tool I have available\\n'\n",
      "            '- Want suggestions for general productivity or decision-making '\n",
      "            'approaches\\n'\n",
      "            '\\n'\n",
      "            'What would be most helpful for you right now?',\n",
      " 'function_call': None,\n",
      " 'provider_specific_fields': {'citations': None, 'thinking_blocks': None},\n",
      " 'role': 'assistant',\n",
      " 'tool_calls': None}\n"
     ]
    }
   ],
   "source": [
    "pprint(response0.choices[0].message.model_dump())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fcd366c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import anthropic\n",
    "client = anthropic.Anthropic()\n",
    "\n",
    "response = client.messages.create(\n",
    "    model=\"claude-sonnet-4-5\",\n",
    "    max_tokens=1024,\n",
    "    tools=[\n",
    "        {\n",
    "            \"name\": \"get_weather\",\n",
    "            \"description\": \"Get the current weather in a given location\",\n",
    "            \"input_schema\": {\n",
    "                \"type\": \"object\",\n",
    "                \"properties\": {\n",
    "                    \"location\": {\n",
    "                        \"type\": \"string\",\n",
    "                        \"description\": \"The city and state, e.g. San Francisco, CA\"\n",
    "                    },\n",
    "                    \"unit\": {\n",
    "                        \"type\": \"string\",\n",
    "                        \"enum\": [\"celsius\", \"fahrenheit\"],\n",
    "                        \"description\": \"The unit of temperature, either \\\"celsius\\\" or \\\"fahrenheit\\\"\"\n",
    "                    }\n",
    "                },\n",
    "                \"required\": [\"location\"]\n",
    "            }\n",
    "        }\n",
    "    ],\n",
    "    messages=[{\"role\": \"user\", \"content\": \"What is the weather like in San Francisco?\"}]\n",
    ")\n",
    "\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "aea582e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def mog(x): return x\n",
    "def zing(y): return y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "c80cfe1b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('mog', <function __main__.mog(x)>),\n",
       " ('zing', <function __main__.zing(y)>),\n",
       " ('mog1', <function __main__.mog(x)>)]"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import collections\n",
    "tools = [mog,zing,mog]\n",
    "n = collections.defaultdict(int)\n",
    "fs = []\n",
    "for f in tools:\n",
    "    i = n[f.__name__]\n",
    "    fs.append((f.__name__+(str(i) if i > 0 else ''), f))\n",
    "    n[f.__name__] = i + 1\n",
    "fs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c3e8acab",
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "import json\n",
    "DUMMY_CHAT_LOGS = '.chats/*.jsonl'\n",
    "files = glob.glob('.chats/*.jsonl')\n",
    "assistant_responses = []\n",
    "for fn in files:\n",
    "    with open(fn, 'r') as f:\n",
    "        for i,txt in enumerate(f.readlines()):\n",
    "            if txt.strip() == '': continue\n",
    "            x = json.loads(txt)\n",
    "            if x['event_type'] != 'transcript_entry': continue\n",
    "            if x['role'] != 'assistant': continue\n",
    "            assistant_responses.append({k:x[k] for k in ['role','content','tool_calls'] if k in x})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "484dbc89",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[PosixPath('.chats/jack-jill.jsonl'),\n",
       " PosixPath('.chats/chat2.jsonl'),\n",
       " PosixPath('.chats/chat1.jsonl'),\n",
       " PosixPath('.chats/chat0.jsonl'),\n",
       " PosixPath('.chats/jack-jill-tragicomic.jsonl'),\n",
       " PosixPath('.chats/chat4.jsonl'),\n",
       " PosixPath('.chats/chat3.jsonl'),\n",
       " PosixPath('.chats/byron-ghandi-2.jsonl'),\n",
       " PosixPath('.chats/weather.jsonl'),\n",
       " PosixPath('.chats/chat5.jsonl'),\n",
       " PosixPath('.chats/jack-jill-haiku.jsonl')]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "list(Path('.').glob(DUMMY_CHAT_LOGS))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "44f76cd0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'a': 2, 'b': 3}"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "{'a':10} | {'b':3, 'a':2}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "238341f2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'{\"n\": -1}'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import json\n",
    "json.dumps({'n':-1})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14d5aa11",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
